# Resumen del Proyecto de Análisis de Datos

Este documento describe la arquitectura, tecnologías y metodologías del proyecto actual, y sirve como base para la implementación de un nuevo sistema utilizando un script de Bash para la automatización y la base de datos `employees` para pruebas de rendimiento.

## 1. Descripción General del Proyecto

El proyecto consiste en una plataforma de análisis de datos contenerizada que integra varias herramientas de código abierto. La plataforma ingiere datos, los procesa y permite su análisis y visualización. La orquestación de los servicios se realiza con Docker Compose.

## 2. Tecnologías Principales

- **Contenerización:** Docker, Docker Compose
- **Base de Datos:** PostgreSQL 10
- **Orquestación de Flujos de Trabajo (ETL):** Apache Airflow 2.9.1
- **Análisis de Datos y Notebooks:** Jupyter Notebook, PySpark, Pandas, Matplotlib, Sympy
- **Monitoreo:** Prometheus
- **Visualización de Métricas:** Grafana
- **Automatización (Propuesta):** Script de Bash

## 3. Arquitectura y Metodología

El sistema sigue una arquitectura de microservicios, donde cada componente se ejecuta en su propio contenedor Docker:

1.  **Base de Datos (`postgres`):** Almacenará la base de datos `employees` para el análisis y la metadata de Airflow.
2.  **Carga de Datos (Nuevo Plan):** Al iniciar, el contenedor de `postgres` ejecutará automáticamente los scripts `.sql` del repositorio `test_db` para crear el esquema y poblar la base de datos `employees`.
3.  **Orquestación (`airflow-*`):** Permitirá definir, programar y monitorear flujos de trabajo (DAGs) para el procesamiento y análisis de los datos de `employees`.
4.  **Entorno de Análisis (`pyspark`):** Proporcionará un entorno interactivo con Jupyter y PySpark para analizar el gran volumen de datos.
5.  **Monitoreo y Visualización (`prometheus`, `grafana`):** Capturarán y mostrarán métricas de rendimiento de la base de datos y los servicios.

## 4. Propuesta de Nueva Implementación

### Automatización con Bash

Un script de Bash (ej. `start.sh`) orquestará el ciclo de vida del entorno, reemplazando la lógica actual de Terraform.

El script `start.sh` se encargará de:
1.  Clonar el repositorio `test_db` si no existe.
2.  Cargar variables de entorno.
3.  Ejecutar los comandos de `docker-compose` para construir y levantar los servicios.

```bash
#!/bin/bash

# Cargar variables de entorno
export $(cat .env | xargs)

# Clonar el repositorio de datos si no existe
if [ ! -d "test_db" ]; then
  git clone https://github.com/datacharmer/test_db.git
fi

# Preparar los scripts de inicialización de la BD
# (Aquí iría la lógica para copiar/adaptar los .sql)

# Detener y limpiar entorno
docker compose down -v

# Construir imágenes
docker compose build

# Levantar servicios
docker compose up -d
```

### Nueva Fuente de Datos: `employees`

- **Repositorio:** `https://github.com/datacharmer/test_db`
- **Plan de Integración:**
    1.  Los scripts `.sql` del repositorio `test_db` se montarán en el directorio `/docker-entrypoint-initdb.d` del contenedor `postgres`.
    2.  El contenedor, en su primer arranque, ejecutará estos scripts para crear la estructura y cargar los datos de la base de datos `employees`.
    3.  Se eliminará el servicio `pgloader`, ya que no será necesario.
    4.  **Posible Desafío:** Será necesario revisar y posiblemente adaptar la sintaxis de los archivos `.sql` de MySQL a PostgreSQL.
